# Lip Movement Reader Application

## About The Project

This project aims to create an innovative application for recognizing and interpreting lip movements, with potential applications in speech recognition, sign language interpretation, and enhanced communication tools. Developed as our graduation project, this application leverages machine learning and computer vision technologies to accurately translate lip movements into text or commands.

The goal is to offer a versatile tool that can aid in various scenarios, from helping those with speech or hearing impairments communicate more effectively to developing more intuitive and interactive technology interfaces.

## Built With

This application is built using a range of programming languages, frameworks, libraries, and tools. Below is a detailed list of the technologies used:

### Programming Language:
- Python

### Frameworks/Libraries:
- TensorFlow
- Keras
- OpenCV (cv2)
- Scikit-learn
- Matplotlib
- Seaborn
- PyTorch (torch)
- torchvision
- numpy
- os
- Mediapipe
- av
- ffmpeg-python

### Tools:
- Figma (for UI/UX design)
- PyCharm (IDE)
- Android Studio (for Android app development)
- GitHub (for version control)
- Flutter (for cross-platform app development)
- Google Colab (for running Jupyter notebooks in the cloud)
- Visual Studio Code (IDE)
